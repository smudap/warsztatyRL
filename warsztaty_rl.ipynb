{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Network (DQN) [[4](#Materia%C5%82y)] jest to adaptacja algorytmu Q-learningu dla głębokich sieci neuronowych. \n",
    "\n",
    "Celem algorytmu jest wykorzystanie głębokiej sieci neuronowej do aproksymacji optymalnej funkcji akcji-wartości (ang. *action-value function*)\n",
    "\n",
    "$$Q^*(s,a) = \\max_{\\pi} \\mathbb{E}(G_t\\ |\\ s_t=s, a_t=a, \\pi) = \\max_{\\pi} \\mathbb{E}(r_t + \\gamma r_{t+1} + \\gamma^2r_{t+2} + \\cdots\\ |\\ s_t=s, a_t=a, \\pi),$$\n",
    "\n",
    "tzn. maksimum sum nagród $r_t$ uwzględniających zniżki $\\gamma$ (ang. *discount factor*) w kolejnych krokach czasu $t$, które są wynikiem wybranych akcji $a$ przez taktykę (ang. *policy*) $\\pi = P(a|s)$ na podstawie obserwacji $s$.\n",
    "\n",
    "Idea jaka stoi za DQN to:\n",
    "1. Wykorzystanie zrandomizowanych powtórek doświadczenia (ang *experience replay*), dzięki czemu usuwa się korelację dla kolejnych obserwacji.\n",
    "2. Iterowane aktualizacje funkcji akcji-wartości $Q$ w kierunku wartości celu $y$ (ang. *target value*), wyznaczonych przez aktualizowaną okresowo (co pewną liczbę kroków) kopię sieci neuronowej z DQN.\n",
    "\n",
    "Algorytm DQN [[4](#Materia%C5%82y)] wygląda następująco:\n",
    "![DQN Algorithm](./images/dqn_algorithm.png)\n",
    "\n",
    "# Double DQN\n",
    "\n",
    "Double DQN [[5](#Materia%C5%82y)] jest modyfikacją algorytmu DQN, dzięki której można zapobiec zbyt optymistycznej estymacji funkcji akcji-wartości $Q$.\n",
    "\n",
    "Modyfikacja ta polega na zmianie liczenia wartości celu $y$, tzn.\n",
    "zamiast \n",
    "\n",
    "$$y_j = r_j + \\gamma \\max_{a'} \\hat{Q}(\\phi_{j+1}, a'; \\theta^{-})$$\n",
    "\n",
    "stosuje się\n",
    "\n",
    "$$y_j = r_j + \\gamma \\hat{Q}(\\phi_{j+1}, \\textit{argmax}_{a'} Q(\\phi_{j+1},a';\\theta); \\theta^{-}).$$\n",
    "\n",
    "Nadal wykorzystuje się tutaj taktykę zachłanną (ang. *greedy policy*) przy wyborze akcji, ale zmienia się zestaw parametrów, które stosuje się dla wyboru akcji.\n",
    "\n",
    "# Dueling DQN\n",
    "\n",
    "Dueling DQN [[5](#Materia%C5%82y)] jest kolejną modyfikacją algorytmu DQN. W tym rozwiązaniu chodzi o nauczenie się, który stan (nie) jest wartościowy bez nauki o efekcie każdej akcji dla każdego stanu (nie zawsze jest sens wyznaczać wartość każdej z akcji, bo część z nich może nie mieć negatywnego wpływu na to co się później wydarzy). W tym celu wykorzystuje się funkcję korzyści (ang. *advantage function*)\n",
    "\n",
    "$$A(s,a) = Q(s,a) - V(s),$$\n",
    "\n",
    "która opisuje istotność wyboru każdej z akcji dla danego stanu $s$.\n",
    "\n",
    "Aby uwzględnić funkcję korzyści w algorytmie, modyfikuje się sieć neuronową zastępując ostatnią warstwę dwoma strumieniami, które odpowiednio wyznaczają funkcję korzyści $A$ oraz wartości $V$, a następnie łączą się w pojedynczy output dla funkcji akcji-wartości $Q$. \n",
    "\n",
    "Sposoby łączenia strumieni to:\n",
    "1. $Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + A(s,a;\\theta,\\alpha)$\n",
    "2. $Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + (A(s,a;\\theta,\\alpha) - \\max_{a'\\in \\mathcal{A}} A(s,a';\\theta,\\alpha))$\n",
    "3. $Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + (A(s,a;\\theta,\\alpha) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'\\in \\mathcal{A}} A(s,a';\\theta,\\alpha))$\n",
    "\n",
    "Porównanie struktury modelu DQN z Dueling DQN na przykładzie konwolucyjnych sieci neuronowych [[5](#Materia%C5%82y)]:\n",
    "![Dueling DQN](./images/dueling_dqn.png)\n",
    "\n",
    "### Uwaga\n",
    "\n",
    "Powyższe modyfikacje można stosować jednocześnie. \n",
    "\n",
    "![DQN Meme](./images/dqn_meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblioteki i funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Permute, Input\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "    \n",
    "def tournament(agent, env, action_repetition=1, nb_episodes=20, nb_max_episode_steps=500, visualize=True):\n",
    "    np.random.seed(nb_episodes)\n",
    "    env.seed(nb_episodes)\n",
    "    tournament_results = agent.test(env, nb_episodes=nb_episodes, action_repetition=action_repetition,\n",
    "                                    nb_max_episode_steps=nb_max_episode_steps, visualize=visualize)\n",
    "    mean_reward = np.mean(tournament_results.history['episode_reward'])\n",
    "    # env.close()\n",
    "    print('Mean reward {}'.format(mean_reward))\n",
    "\n",
    "def logs_visualizer(log_path, train=True, stats=None, fig_size=None, save_to_file=False):\n",
    "    with open(log_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = pd.DataFrame(data)\n",
    "    if train:\n",
    "        which_rows = data.isna().apply(lambda row: not pd.np.any(row), axis=1).tolist()\n",
    "        data = data.loc[which_rows]\n",
    "    episodes = data['episode']\n",
    "    if stats is None:\n",
    "        stats = ['duration', 'nb_episode_steps', 'episode_reward']\n",
    "    if train:\n",
    "        stats += ['mean_q', 'mean_eps']\n",
    "\n",
    "    n = len(stats)\n",
    "    if fig_size is None:\n",
    "        fig_size = (15., 5. * n)\n",
    "\n",
    "    _, plots = plt.subplots(nrows=n, sharex=True, figsize=fig_size)\n",
    "    for i, stat in enumerate(stats):\n",
    "        plots[i].plot(episodes, data[stat])\n",
    "        plots[i].set_ylabel(stat, fontsize=20)\n",
    "        plots[i].tick_params(labelsize=15)\n",
    "    plt.xlabel('episodes', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if save_to_file:\n",
    "        plt.savefig(log_path[:-4] + 'pdf')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguracja gry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole | MountainCar\n",
    ":----:|:----:\n",
    "![CartPole](./images/cartpole.png)  |  ![MountainCar](./images/mountaincar.png)\n",
    "\n",
    "\n",
    "#### OpenAI Gym Wiki\n",
    "- [CartPole](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "- [MountainCar](https://github.com/openai/gym/wiki/MountainCar-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "# env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "training_seed = 20\n",
    "env.seed(training_seed)\n",
    "np.random.seed(training_seed)\n",
    "nb_actions = env.action_space.n\n",
    "space_shape = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametry do modyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "`window_length` - ilość kolejnych obrazków branych jako input; w grach 2 klatki odpowiadają za ruch, 3 za prędkość\n",
    "\n",
    "#### Memory Replay\n",
    "\n",
    "`memory_limit` - maksymalna liczba zapamiętanych ostatnich klatek gry\n",
    "\n",
    "#### Policy\n",
    "\n",
    "Do wyboru z policy mamy:  \n",
    "`BoltzmannQPolicy` - Boltzmann Policy opiera się na prawdopodobieństwach wyznaczonych na podstawie Q wartości. Zwraca wylosowaną akcję na podstawie obliczonych prawdopodobieństw.  \n",
    "\n",
    "`EpsGreedyQPolicy` - $\\varepsilon$-Greedy Policy wybiera akcję według schematu:  \n",
    "1. Losowana jest liczba $x\\sim U([0,1])$\n",
    "2. Wybierana jest akcja $$a = \\cases{\\text{losowa akcja} & \\text{ dla } x < \\varepsilon\\\\ \\textit{argmax}_{a'\\in A}Q(s,a') & \\text{ dla } x \\ge \\varepsilon}$$  \n",
    "\n",
    "`LinearAnnealedPolicy` - Linear Annealed Policy wykorzystuje $\\varepsilon$-Greedy Policy, gdzie dla danego kroku $t$ $\\varepsilon_t$ wyznaczony jest według wzoru:\n",
    "$$\\varepsilon_t = \\max{\\left(\\varepsilon_{min}, -\\frac{\\varepsilon_{max} - \\varepsilon_{min}}{\\text{nb_steps_annealing}}\\cdot \\text{nb_step}_t + \\varepsilon_{max}\\right)}$$  \n",
    "\n",
    "Parametry w policy:\n",
    "\n",
    "`eps_training_max` - początkowa wartość $\\varepsilon$  \n",
    "`eps_training_min` - końcowa wartość $\\varepsilon$  \n",
    "`eps_test` - wartość $\\varepsilon$ dla testowania agenta  \n",
    "`nb_steps_annealing` - liczba kroków potrzebnych do otrzymania `eps_training_min`\n",
    "\n",
    "#### Agent\n",
    "\n",
    "`double_dqn` - czy ma być wykorzystany Double DQN  \n",
    "`dueling_dqn` - czy ma być wykorzystany Dueling DQN  \n",
    "`dueling_type` - rodzaj agregacji w Dueling DQN  \n",
    "`nb_steps_learning` - liczba kroków nauki agenta  \n",
    "`nb_steps_warmup` - liczba kroków rozgrzewkowych, które nie wpływają na aktualizację modelów w agencie  \n",
    "`nb_steps_target_model_update` - co ile kroków ma być zaktualizowany model do wyznaczania targetów  \n",
    "`batch_size` - rozmiar batchy  \n",
    "`gamma` - `discount factor` $\\gamma$ z Q Learning  \n",
    "`train_interval` - co ile kroków model ma się uczyć   \n",
    "`action_repetition` - ile razy ma być wykonana pojedyncza akcja  \n",
    "`optimizer_learning_rate` - learning rate w optimizerze (Adam) sieci neuronowej (śmiało można zmienić Adama na coś innego)  \n",
    "`optimizer_metrics` - lista zawierająca miary w optimizerze (Adam) sieci neuronowej (śmiało można zmienić Adama na coś innego)   \n",
    "`nb_steps_log` - co ile kroków mają być logowane postępy oraz zapisywane wagi modelów  \n",
    "`visualize_learning` - czy gra ma być wyświetlana podczas nauki  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "window_length = 1\n",
    "\n",
    "# Memory Replay\n",
    "memory_limit = 50000\n",
    "\n",
    "# Policy\n",
    "eps_training_max = 1\n",
    "eps_training_min = 0.1\n",
    "eps_test = 0.05\n",
    "nb_steps_annealing = 25000\n",
    "\n",
    "# Agent\n",
    "double_dqn = False\n",
    "dueling_dqn = False\n",
    "dueling_type = 'avg'\n",
    "nb_steps_learning = 50000\n",
    "nb_steps_warmup = 5000\n",
    "nb_steps_target_model_update = 1000\n",
    "batch_size=32\n",
    "gamma=.99\n",
    "train_interval=1\n",
    "action_repetition = 1\n",
    "optimizer_learning_rate = 0.01\n",
    "optimizer_metrics = ['mae']\n",
    "nb_steps_log = 10000\n",
    "visualize_learning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykładowy model - można go dowolnie modyfikować"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (window_length,) + space_shape\n",
    "model = Sequential() \n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print('Input shape: {}'.format(input_shape))\n",
    "print('Output shape: {}'.format(nb_actions))\n",
    "# (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nauka agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Replay\n",
    "memory = SequentialMemory(limit=memory_limit, \n",
    "                          window_length=window_length)\n",
    "\n",
    "# Policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', \n",
    "                              value_max=eps_training_max, \n",
    "                              value_min=eps_training_min, \n",
    "                              value_test=eps_test,\n",
    "                              nb_steps=nb_steps_annealing)\n",
    "\n",
    "# Agent\n",
    "dqn = DQNAgent(model=model, \n",
    "               memory=memory, \n",
    "               policy=policy,\n",
    "               enable_double_dqn=double_dqn, \n",
    "               enable_dueling_network=dueling_dqn, \n",
    "               dueling_type=dueling_type,\n",
    "               nb_actions=nb_actions, \n",
    "               nb_steps_warmup=nb_steps_warmup,\n",
    "               target_model_update=nb_steps_target_model_update,\n",
    "               batch_size=batch_size,\n",
    "               gamma=gamma,\n",
    "               train_interval=train_interval)\n",
    "dqn.compile(Adam(lr=optimizer_learning_rate), \n",
    "            metrics=['mae'])\n",
    "\n",
    "# Logs & weights\n",
    "weights_filename = 'weights/{}_weights.h5f'.format(env_name)\n",
    "log_filename = 'logs/{}_log.json'.format(env_name)\n",
    "checkpoint_weights_filename = 'weights/' + env_name + '_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=nb_steps_log)]\n",
    "callbacks += [FileLogger(log_filename, interval=nb_steps_log)]\n",
    "\n",
    "# Learning\n",
    "print('Start learning: ' + time.asctime())\n",
    "dqn.fit(env, \n",
    "        callbacks=callbacks, \n",
    "        nb_steps=nb_steps_learning, \n",
    "        log_interval=nb_steps_log, \n",
    "        action_repetition=action_repetition, \n",
    "        visualize=visualize_learning)\n",
    "print('End learning: ' + time.asctime())\n",
    "logs_visualizer('./logs/CartPole-v0_log.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turniej Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zasady\n",
    "\n",
    "Turniej składa się z 20 gier. Zwycięzcą zostaje osoba, która uzyska średnio najwyższy wynik.\n",
    "\n",
    "Nagrody:\n",
    "* +1 za przeżycie w danym kroku\n",
    "\n",
    "Uwaga:\n",
    "Ziarna generatorów liczb pseudolosowych są ustawione na liczbę gier (20).\n",
    "\n",
    "### Parametry do modyfikacji\n",
    "\n",
    "`action_repetition` - ile razy ma być wykonana pojedyncza akcja  \n",
    "`visualize` - czy gra ma być wizualizowana\n",
    "\n",
    "### Uwaga\n",
    "\n",
    "Istnieje możliwość wczytania wcześniejszych wersji nauczonych wag modelu. Można to zrobić metodą `agent.load_weights(filepath)`, która jako argument przyjmuje ścieżkę do pliku z wagami modelu, np.:\n",
    "```python\n",
    "dqn.load_weights('./weights/last_weights.h5f')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_action_repetition = 1\n",
    "tournament_visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament(dqn, env, tournament_action_repetition, visualize=tournament_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materiały"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [UCL Course on RL - David Silver, 2015](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "2. [Deep RL Bootcamp, sierpień 2017](https://sites.google.com/view/deep-rl-bootcamp/)\n",
    "3. [Reinforcement Learning: An Introduction (draft książki, maj 2018), Richard S. Sutton and Andrew G. Barto](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view)\n",
    "4. [(DeepMind DQN) Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "5. [(DeepMind Double DQN) Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "6. [(DeepMind Dueling DQN) Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "7. [OpenAI Gym](https://gym.openai.com)\n",
    "8. [OpenAI Gym Github](https://github.com/openai/gym)\n",
    "9. [OpenAI Baselines Github](https://github.com/openai/baselines)\n",
    "10. [keras-rl Github](https://github.com/keras-rl/keras-rl)\n",
    "11. [TensorForce Github](https://github.com/reinforceio/tensorforce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
