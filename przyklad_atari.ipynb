{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblioteki i funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Permute, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguracja gry Riverraid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Riverraid](./images/riverraid.png)\n",
    "\n",
    "\n",
    "#### OpenAI Gym\n",
    "- [Riverraid-v0](https://gym.openai.com/envs/Riverraid-v0/) (wersja `Riverraid-v4` pomija 4 klatki/powtarza akcję 4 razy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Riverraid-v4'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametry i model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape)) # (width, height, channels)\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print('Input shape: {}'.format(input_shape))\n",
    "print('Output shape: {}'.format(nb_actions))\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nauka agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AtariProcessor()\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, \n",
    "                          window_length=WINDOW_LENGTH)\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', \n",
    "                              value_max=1, \n",
    "                              value_min=0.1, \n",
    "                              value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               policy=policy, \n",
    "               memory=memory,\n",
    "               processor=processor, \n",
    "               enable_double_dqn=True,\n",
    "               enable_dueling_network=False, \n",
    "               nb_steps_warmup=500, \n",
    "               gamma=.99, \n",
    "               target_model_update=100,\n",
    "               train_interval=4, \n",
    "               delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), \n",
    "            metrics=['mae'])\n",
    "\n",
    "weights_filename = 'weights/{}_weights.h5f'.format(ENV_NAME)\n",
    "log_filename = 'logs/{}_log.json'.format(ENV_NAME)\n",
    "checkpoint_weights_filename = 'weights/' + ENV_NAME + '_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.fit(env, \n",
    "#         callbacks=callbacks, \n",
    "#         nb_steps=2000000, \n",
    "#         log_interval=100000, \n",
    "#         visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn.load_weights(filepath='./weights/Riverraid-v4_weights.h5f')\n",
    "dqn.test(env, nb_episodes=10, action_repetition=1, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguracja gry Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Breakout](./images/breakout.png)\n",
    "\n",
    "\n",
    "#### OpenAI Gym\n",
    "- [Breakout-v0](https://gym.openai.com/envs/Breakout-v0/) (wersja `Breakout-v4` pomija 4 klatki/powtarza akcję 4 razy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Breakout-v4'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametry i model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape)) # (width, height, channels)\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print('Input shape: {}'.format(input_shape))\n",
    "print('Output shape: {}'.format(nb_actions))\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nauka agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AtariProcessor()\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, \n",
    "                          window_length=WINDOW_LENGTH)\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', \n",
    "                              value_max=1, \n",
    "                              value_min=0.1, \n",
    "                              value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               policy=policy, \n",
    "               memory=memory,\n",
    "               processor=processor, \n",
    "               enable_double_dqn=True,\n",
    "               enable_dueling_network=False,\n",
    "               nb_steps_warmup=500, \n",
    "               gamma=.99, \n",
    "               target_model_update=100,\n",
    "               train_interval=4, \n",
    "               delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), \n",
    "            metrics=['mae'])\n",
    "\n",
    "weights_filename = 'weights/{}_weights.h5f'.format(ENV_NAME)\n",
    "log_filename = 'logs/{}_log.json'.format(ENV_NAME)\n",
    "checkpoint_weights_filename = 'weights/' + ENV_NAME + '_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.fit(env, \n",
    "#         callbacks=callbacks, \n",
    "#         nb_steps=2000000, \n",
    "#         log_interval=100000, \n",
    "#         visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn.load_weights(filepath='./weights/Breakout-v4_weights.h5f')\n",
    "dqn.test(env, nb_episodes=10, action_repetition=1, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Źródło"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [keras-rl Github](https://github.com/keras-rl/keras-rl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
